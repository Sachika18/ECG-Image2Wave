================================================================================
ADVANCED INTERVIEW QUESTIONS - ECG DIGITIZER PROJECT
================================================================================

These are deeper, more challenging questions an interviewer might ask to assess
your understanding of the project, problem-solving, and ML knowledge.


================================================================================
SECTION 1: MODEL ARCHITECTURE & DESIGN DECISIONS
================================================================================

Q: Why did you choose ResNet18 specifically? Why not ResNet50, MobileNet, or Vision Transformer?

A: ResNet18 considerations:
   - Pros:
     * 18 layers is balanced: deep enough for feature extraction, not too deep
     * Skip connections solve vanishing gradient problem
     * Well-established, proven architecture for image tasks
     * Relatively fast inference (fewer parameters than ResNet50)
     * Good accuracy/speed tradeoff for real-time applications
   
   - ResNet50 cons: 50 layers = more parameters, slower inference, overkill for ECG panels
   - MobileNet pros: lighter, but ECG is specialized task, generic lightweight archits underperform
   - Vision Transformer cons: requires huge amounts of training data, slow, not needed for simple ECG
   
   - Decision: ResNet18 provides good balance of expressiveness and efficiency


Q: Explain the mean pooling fusion strategy. What are its limitations?

A: Current approach:
   - Take 12 panels, each output 512-d feature
   - Average across 12 panels: (12, 512) → (512,)
   - Pass to decoder
   
   Limitations:
   1. Information loss: averaging destroys panel-specific patterns
      - If panel 5 has unique feature, it's diluted by 11 others
      - Can't distinguish which panel contains which information
   
   2. No attention mechanism: treats all panels equally
      - Some panels may be more informative than others
      - Model can't learn to weight them
   
   3. Permutation invariance: order doesn't matter
      - Shuffling panels produces same output
      - Real ECG panels have order/sequence significance
   
   Better alternatives:
   - Attention pooling: learn weights for each panel
   - Concatenation: preserve panel identity (more parameters)
   - Hierarchical fusion: gradually merge pairs of panels
   - Gating mechanism: learn which panels to trust


Q: Why use ConvTranspose1d (deconvolution) in the decoder instead of upsampling + Conv1d?

A: ConvTranspose1d:
   - Learnable upsampling with learned patterns
   - Can learn complex smooth transitions in waveform
   - Direct inverse of normal convolution (theoretically sound)
   
   Upsampling + Conv1d alternative:
   - Upsample (e.g., repeat pixels): (B, C, T) → (B, C, 2T)
   - Then apply Conv1d to refine
   - Cons: 
     * Two-stage process, less end-to-end learnable
     * Upsampling is often too simplistic (leads to artifacts)
   
   Benefits of ConvTranspose1d:
   - Stride parameter directly controls upsampling factor
   - Learnable initialization (not fixed repeating)
   - Better gradient flow during backprop
   - More compact representation


Q: The decoder outputs (B, 12, 10250). How does the model ensure waveforms are physiologically realistic?

A: Current approach: No explicit constraints
   - Model just learns to minimize MSE loss
   - No guarantees waveforms are physically meaningful
   
   Issues that could occur:
   - Unrealistic amplitudes (too high/low)
   - Discontinuities or noise in output
   - Leads don't follow cardiac rhythm relationships
   - QRS complexes in wrong positions
   
   How to improve:
   1. Domain-specific loss:
      - Penalty for high-frequency noise: L2 on gradients
      - Amplitude clipping: enforce min/max per lead
      - Cross-lead correlation loss: V1-V6 should be correlated
   
   2. Post-processing:
      - Smooth output with low-pass filter
      - Clip to realistic range
      - Apply median filtering
   
   3. Validation during training:
      - Check outputs against signal processing standards
      - Compare spectral properties with real ECGs
   
   4. Adversarial training:
      - Add discriminator to check if output looks real
      - GAN-style approach


Q: What happens if someone uploads images that are completely different from training data (e.g., X-ray instead of ECG)?

A: The model will still produce output (no error checking)
   
   Problems:
   - Output will be meaningless (random noise-like waveform)
   - No way to know it's wrong from the output alone
   - User might trust the result
   - Safety issue for medical applications
   
   Solutions:
   1. Input validation:
      - Check image properties (size, color distribution)
      - Reject if doesn't look like ECG
   
   2. Confidence score:
      - Add classification head: is this an ECG? (0-1)
      - Warn user if confidence < threshold
   
   3. Output validation:
      - Check if output matches ECG patterns
      - Flag if amplitude/frequency unrealistic
   
   4. User education:
      - Clear instructions on what images to upload
      - Example images provided
   
   5. Disclaimer:
      - Add legal warning: "For research only, not clinical use"


================================================================================
SECTION 2: TRAINING & OPTIMIZATION
================================================================================

Q: Why use AdamW instead of SGD?

A: Adam (Adaptive Moment Estimation):
   - Adapts learning rate per parameter
   - Uses momentum and RMSprop (adaptive scaling)
   - Pros:
     * Faster convergence (fewer epochs)
     * Works well with sparse gradients
     * Less sensitive to hyperparameter tuning
     * Good default choice for most problems
   
   AdamW (Adam with decoupled Weight Decay):
   - Standard Adam had poor weight decay implementation
   - AdamW fixes this by decoupling L2 regularization
   - Pros: Better generalization, cleaner weight decay
   
   SGD alternative:
   - Pros: Often better final accuracy with tuning
   - Cons: Need careful LR scheduling, slower convergence
   
   For this project: AdamW is good choice
   - ECG prediction is non-convex problem
   - Benefits from adaptive learning rates
   - Weight decay helps prevent overfitting


Q: What is the learning rate 1e-4 and why not use something higher or lower?

A: Learning rate (LR) = 0.0001 = 1e-4
   
   Role of LR:
   - Controls step size of weight updates
   - LR = (new_weight - old_weight) / step_size
   - Too high: diverges, loss explodes
   - Too low: converges slowly, may get stuck
   
   Why 1e-4 for this project:
   - ResNet18 has 11M parameters, UNet has many more
   - Deep networks need small LR (gradient scaling)
   - 1e-4 is conservative but reliable
   - Typical range for Adam: [1e-5, 1e-3]
   
   Comparison:
   - 1e-3 (0.001): Too high, likely to diverge or oscillate
   - 1e-4 (0.0001): Good starting point, safe
   - 1e-5 (0.00001): Very slow, but stable
   
   Best practice: Start with 1e-4, monitor loss curve
   - If diverging: decrease to 1e-5
   - If too slow: increase to 5e-4
   - Use learning rate scheduling for adaptive updates


Q: What does weight decay = 1e-5 do?

A: Weight Decay = L2 Regularization
   - Adds penalty term: loss = MSE + weight_decay * sum(weights^2)
   - Encourages weights to stay small
   - Formula: weight_update *= (1 - weight_decay * lr)
   
   Why use it:
   - Prevents overfitting (large weights = memorization)
   - Improves generalization to unseen data
   - Acts as implicit prior (weights ~ Normal(0, std))
   
   Effect of 1e-5:
   - Mild penalty (not too aggressive)
   - Allows model to learn strong features when needed
   - Doesn't over-constrain weights
   
   Comparison:
   - 0: No regularization, may overfit
   - 1e-5: Good default, balance
   - 1e-4: Stronger, weights stay small but may underfit
   - 1e-3: Too strong, prevents learning


Q: You used batch size 1. Why not batch size 4, 8, or 16?

A: Batch size = 1 (One sample per update)
   
   Considerations:
   - Memory: Each sample needs (B, 12, 3, 128, 128) = massive
     * B=1: 12*3*128*128 = 6M pixels = ~72MB per image
     * B=4: 4x memory, may cause OOM
   
   - Training speed:
     * B=1: 1 update per batch (slow)
     * B=4: 4 updates per batch (faster but different gradient)
   
   - Gradient quality:
     * B=1: Noisy gradients (high variance)
     * B=4+: Smoother gradient estimates
     * Trade-off: B=1 is noisier but regularizing
   
   Why B=1 here:
   - Memory constraints (GPU/CPU limited)
   - ECG prediction doesn't need large batches
   - Noise in gradients can help escape local minima
   
   Better approach (if memory allows):
   - B=2 or B=4: Better gradient estimates
   - Accumulate gradients: simulate B=4 by updating every 4 steps
   - But for this project, B=1 is practical choice


Q: Why save only the best checkpoint and not use early stopping?

A: Current approach:
   ```
   if ep_loss < best_loss:
       torch.save(model.state_dict(), "best.pth")
   ```
   
   This is early stopping variant:
   - Save best, but continue training all 10 epochs
   - Return best weights at end
   
   Pure early stopping:
   - Stop training when validation loss doesn't improve for N epochs
   - Example: patience=3
   ```
   if val_loss < best_loss:
       best_loss = val_loss
       patience_counter = 0
   else:
       patience_counter += 1
       if patience_counter >= 3:
           break  # Stop training
   ```
   
   Pros/Cons:
   - Checkpoint saving: Continue training, always have best
     * Pro: Exploration, sometimes later epochs improve
     * Con: Might overfit eventually
   
   - Early stopping: Stop immediately when stuck
     * Pro: Prevents overfitting
     * Con: Might stop too early, miss improvements
   
   Best practice:
   - Use both: Save best + early stopping
   - Check validation loss (not training loss) for early stopping
   - This project only uses train loss (should use validation)


================================================================================
SECTION 3: DEBUGGING & TROUBLESHOOTING
================================================================================

Q: How did you identify that the checkpoint wasn't loading?

A: Detection process:
   1. First symptom: RuntimeError about missing keys
   2. Used diagnose_checkpoint.py to inspect:
      - Listed all checkpoint keys (165)
      - Listed all model keys (165 after fix)
      - Compared and found mismatches
   
   3. Created verify_loading.py to check:
      - Compare weight values before/after loading
      - If weights don't change: loading failed
      - If weights change significantly: loading worked
   
   4. Then tested predictions:
      - Same images produced identical outputs
      - Indicated model not responding to input
      - Suggested weights weren't meaningful (or random)


Q: What would you do if checkpoint had partial key mismatches?

A: Strategy:
   1. Identify which keys are mismatched
   2. Map old key names to new names if naming changed
      ```
      key_mapping = {
          'encoder.features.0': 'encoder.conv1',  # hypothetical
          'decoder.proj': 'decoder.project'
      }
      ```
   
   3. Try several approaches:
      a) Use strict=False (skip unmatched keys) ← used in project
      b) Manually map keys before loading
      c) Fine-tune from scratch with new architecture
      d) Transfer learning from partial checkpoint
   
   4. Verify with diagnostics:
      - Check which keys loaded vs skipped
      - Verify weight values changed
      - Test output makes sense


Q: The model produces similar outputs for different images. How would you debug this?

A: Systematic debugging:
   1. Check encoder features:
      - Extract intermediate features from panels
      - Are different images producing different encodings?
      - If all same → problem in encoder
   
   2. Check fusion layer:
      - What does averaged feature look like?
      - Is it compressing useful information?
      - Try skipping fusion (pass first panel only)
   
   3. Check decoder:
      - Feed different random latent vectors
      - Does decoder produce different outputs?
      - If yes → decoder works, problem in encoder/fusion
      - If no → decoder is stuck (learned constant output)
   
   4. Check loss during training:
      - Did loss actually decrease?
      - Or just random initialization that happened to work?
      - Plot training curves
   
   5. Verify checkpoint loaded:
      - Compare weights before/after loading
      - Inspect weight statistics (mean, std)
   
   6. Test with synthetic data:
      - Create very different images (red vs blue)
      - If model still produces same output → real problem


================================================================================
SECTION 4: DEPLOYMENT & SCALABILITY
================================================================================

Q: How would you deploy this model to production?

A: Multiple options:
   
   Option 1: Streamlit Cloud (simplest)
   - Push code to GitHub
   - Connect to Streamlit Cloud
   - App runs on their servers
   - Pros: Easy, free tier available
   - Cons: Limited resources, slow for high traffic
   
   Option 2: Docker container on cloud
   - Dockerize Streamlit app
   - Deploy to AWS/GCP/Azure
   - More control, scalable
   - Pros: Production-grade, autoscaling
   - Cons: More complex, costs money
   
   Option 3: REST API
   - Use FastAPI instead of Streamlit
   - Create /predict endpoint
   - Deploy as microservice
   - Pros: Scalable, can serve many requests
   - Cons: More backend work
   
   Option 4: Mobile app
   - Convert model to ONNX/TFLite
   - Deploy on phone
   - Pros: Offline, low latency
   - Cons: Size constraints, low compute
   
   For this project:
   - Start with Streamlit Cloud (quick)
   - Move to Docker + Kubernetes (scale)
   - Add REST API if needed


Q: How would you optimize inference speed?

A: Current bottlenecks:
   1. Image resizing: 128x128 → fast
   2. ResNet18 forward: ~50ms on CPU, ~5ms on GPU
   3. Fusion averaging: ~1ms
   4. Decoder upsampling: ~30ms
   5. Total: ~80ms on CPU, ~40ms on GPU
   
   Optimization strategies:
   
   1. Model quantization:
      - Convert FP32 → INT8
      - 4x faster inference, smaller model
      - torch.quantization.quantize_dynamic()
   
   2. Knowledge distillation:
      - Train smaller student model from teacher
      - Smaller model runs faster
   
   3. Model pruning:
      - Remove unimportant weights
      - 30-50% speedup possible
   
   4. Batch inference:
      - Process multiple requests together
      - Parallelize across GPU cores
   
   5. ONNX export:
      - Convert to ONNX format
      - Optimize with ONNX Runtime
      - Better performance than PyTorch
   
   6. GPU deployment:
      - Run on GPU instead of CPU
      - 10x speedup typical


Q: How would you handle high traffic (many users uploading simultaneously)?

A: Scaling strategies:
   
   Single instance:
   - Current: Streamlit handles one request at a time
   - Problem: queue builds up, users wait
   - Solution: Nothing you can do in Streamlit
   
   Multiple instances:
   - Deploy multiple Streamlit apps
   - Load balancer distributes requests
   - Each gets subset of users
   
   Message queue:
   - Users submit requests to queue (Redis, RabbitMQ)
   - Workers (multiple processes) consume and process
   - Async responses to users
   
   Database caching:
   - Cache predictions for common image patterns
   - If user uploads similar image, return cached result
   - Huge speedup for repeated inputs
   
   Model inference server:
   - Replace Streamlit with FastAPI
   - Multiple worker processes
   - True parallel inference
   
   Serverless:
   - AWS Lambda, Google Cloud Functions
   - Auto-scales with traffic
   - Pay only for compute time


================================================================================
SECTION 5: MACHINE LEARNING FUNDAMENTALS
================================================================================

Q: Explain the difference between training, validation, and test sets.

A: Three dataset splits:
   
   1. Training set (~70%):
      - Used to update model weights
      - Compute loss, backprop, SGD
      - Model "learns" from this
      - Problem: Can overfit (memorize)
   
   2. Validation set (~15%):
      - Used to tune hyperparameters
      - Evaluate during training (not for updates)
      - Check for overfitting
      - Used for early stopping
      - Problem: Can overfit validation too (leakage)
   
   3. Test set (~15%):
      - Used only at the very end
      - Never seen during training/tuning
      - True measure of generalization
      - Should be kept secret until final evaluation
      - Problem: None, if used correctly
   
   In this project:
   - Only using training set (should split!)
   - No validation or test sets
   - Can't reliably estimate generalization


Q: What is overfitting? How would you detect and prevent it?

A: Overfitting = Model learns training data perfectly but fails on new data
   
   Cause: Model has too many parameters relative to training data
   
   Example:
   - Model memorizes all 50 training samples
   - Training loss = 0.001
   - But validation loss = 10 (very high)
   - Clear sign of overfitting
   
   Detection methods:
   1. Compare training vs validation loss
      - If val_loss >> train_loss → overfitting
   
   2. Early stopping:
      - Monitor val_loss during training
      - If not improving for N epochs → stop
   
   3. Cross-validation:
      - Test on multiple hold-out sets
      - If average performance varies → overfitting
   
   Prevention strategies:
   1. More training data: Hardest to get, most effective
   2. Regularization: L1/L2 weight penalties
   3. Dropout: Random neuron deactivation
   4. Early stopping: Stop when val_loss plateaus
   5. Simpler model: Fewer parameters
   6. Data augmentation: Synthetic variations
   7. Ensemble: Combine multiple models


Q: What is batch normalization and why does it help?

A: Batch Normalization (BatchNorm):
   - Normalizes inputs to each layer
   - Formula: x_norm = (x - batch_mean) / batch_std
   - Then apply learned scale/shift: y = γ * x_norm + β
   
   Why it helps:
   
   1. Internal covariate shift:
      - As earlier layers change, inputs to deeper layers shift
      - Batch norm stabilizes this
   
   2. Faster training:
      - Can use higher learning rates
      - Gradients more stable
      - Convergence faster
   
   3. Regularization effect:
      - Noise from batch statistics acts as regularizer
      - Reduces overfitting
      - Alternative to dropout
   
   4. More stable gradients:
      - Prevents vanishing/exploding gradients
      - Deeper networks train better
   
   In this project:
   - Used in ResNet18 (BN after conv layers)
   - Used in decoder (BN after deconv + ReLU)
   - Helps model train faster and generalize better


Q: Explain the vanishing gradient problem and how ResNets solve it.

A: Vanishing Gradient Problem:
   - In deep networks, gradients propagate backwards
   - Through many layers, gradient gets multiplied by many small values
   - Eventually becomes extremely small (vanishing)
   - Model can't update early layers
   
   Effect:
   - Early layers don't learn useful features
   - Only deep layers contribute
   - Can't train networks beyond ~15-20 layers efficiently
   
   Solution: Residual Connections (ResNet):
   ```
   y = x + f(x)  # Skip connection
   vs
   y = f(x)      # Normal layer
   ```
   
   Why it works:
   - Gradient can flow directly through skip connection
   - dL/dx = dL/dy * (1 + df(x)/dx)
   - If df/dx small, gradient still 1 from skip connection
   - Gradient doesn't vanish
   
   Effect:
   - Can train 50, 100, 150+ layer networks
   - ResNet18 is relatively shallow (benefits less, still helps)
   - ResNet50 benefits more from skip connections


================================================================================
SECTION 6: PROJECT-SPECIFIC QUESTIONS
================================================================================

Q: Why is the checkpoint file named "resnet_unet_best.pth"?

A: Naming convention:
   - "resnet_unet" = Architecture used
   - "best" = Best model during training (not last epoch)
   - ".pth" = PyTorch file extension
   
   Good naming practices:
   - Include model name
   - Include quality metric (best, final, checkpoint_10)
   - Include version if multiple experiments
   - Examples: "resnet_v2_best.pth", "resnet_unet_lr0001_best.pth"


Q: If you had to retrain this model, what would you do differently?

A: Improvements:
   
   1. Data handling:
      - Split into train/val/test sets
      - Use validation set for early stopping
      - Monitor on validation, not training
   
   2. Diverse training data:
      - Include ECGs from multiple sources, diseases
      - Various image qualities, rotations, scales
      - Data augmentation for robustness
   
   3. Architecture improvements:
      - Replace mean pooling with attention
      - Add skip connections between encoder/decoder
      - Larger decoder (more parameters)
      - Per-lead prediction heads (one decoder per lead)
   
   4. Loss improvements:
      - Add perceptual loss (signal quality metrics)
      - Frequency-domain loss (compare FFT)
      - Adversarial loss (GAN discriminator)
      - Cross-lead correlation loss
   
   5. Regularization:
      - Dropout in decoder
      - Data augmentation
      - Weight decay (already used)
   
   6. Training optimization:
      - Larger batch sizes (if memory allows)
      - Learning rate scheduling
      - Gradient accumulation for large batches
      - Mixed precision training (already used)
   
   7. Evaluation:
      - Use proper validation metrics (not just loss)
      - Cross-validation
      - Test on unseen hospital data


Q: How would you make this model explainable (interpretable)?

A: Explainability techniques:
   
   1. Feature visualization:
      - Visualize what ResNet learned
      - GradCAM: highlight important image regions
      - Shows which panel areas matter most
   
   2. Attention weights:
      - If using attention pooling: visualize panel weights
      - Shows which panels contributed most to prediction
   
   3. Grad-based analysis:
      - Input gradients: show sensitive pixels
      - Which image changes affect output most?
   
   4. LIME (Local Interpretable Model-agnostic Explanations):
      - Approximate model locally
      - Show which features matter
   
   5. SHAP (SHapley Additive exPlanations):
      - Game theory approach
      - Quantify each feature contribution
   
   6. Ablation studies:
      - Remove panels one by one
      - See how output changes
      - Identifies important panels
   
   Clinical validation:
   - Compare predicted waveforms to expert annotations
   - Check if model mistakes match known difficult cases
   - Build confidence through validation


================================================================================
SECTION 7: REALISTIC SCENARIO QUESTIONS
================================================================================

Q: Imagine your model predicts a dangerous arrhythmia pattern that's incorrect. 
   What went wrong and how do you fix it?

A: Root cause analysis:
   1. Check input image quality
      - Was it clear ECG or blurry/rotated?
      - Is it even an ECG or wrong image type?
   
   2. Check model training
      - Does training data include this arrhythmia type?
      - Is model trained to recognize it?
   
   3. Check prediction pipeline
      - Did model confidence drop (if available)?
      - Can you retrace through intermediate features?
   
   4. Check evaluation
      - Is this a one-off error or systematic?
      - Test on similar images
   
   Prevention:
   1. Add confidence score
   2. Flag predictions that don't match expected patterns
   3. Manual review process for critical cases
   4. Uncertainty quantification (Bayesian)
   5. Better training data with this case
   6. Retraining with corrected labels
   
   Long-term:
   - Explainability: Show why model predicted this
   - Robustness testing: Test on adversarial examples
   - Clinical validation: Compare to cardiologist


Q: Your model works on your test images but fails in production. Why?

A: Common causes of distribution shift:
   
   1. Data differences:
      - Production images different quality/camera/angle
      - Training data didn't cover these variations
      - Solution: Collect production data, retrain
   
   2. Image preprocessing:
      - Different image formats (RGB vs grayscale)
      - Different resolutions
      - Different normalization standards
      - Solution: Standardize preprocessing
   
   3. Model issues:
      - Overfitted to training environment
      - Batch norm statistics outdated
      - Solution: Collect more diverse training data
   
   4. Environmental:
      - Different hardware (GPU vs CPU differences)
      - Different PyTorch versions
      - Solution: Test on target hardware
   
   5. User error:
      - Uploading wrong image format
      - Uploading non-ECG images
      - Solution: Better validation, user guidance
   
   Debugging approach:
   - Get failing examples from production
   - Compare to training distribution
   - Add logging to see intermediate values
   - A/B test new model on subset


================================================================================
SECTION 8: OPEN-ENDED QUESTIONS
================================================================================

Q: If given unlimited time and resources, what would be your dream version of this project?

A: Full-stack medical AI system:
   
   Model improvements:
   - Multi-task learning: Predict rhythm, disease, quality score
   - Self-supervised pretraining: Learn from unlabeled ECGs
   - Federated learning: Train across hospitals without sharing data
   - Uncertainty quantification: Know when to ask human
   - Continual learning: Adapt to new data over time
   
   Data:
   - Millions of diverse ECGs (multiple countries, conditions)
   - Clinical outcomes (what happened to patient after)
   - Long-term follow-up (did model predictions match reality)
   
   Deployment:
   - Mobile app with offline capability
   - Cloud API with REST endpoints
   - Real-time alerts for critical findings
   - Integration with hospital EHR systems
   
   Validation:
   - Clinical trials: Compare to cardiologists
   - FDA approval: Regulatory clearance
   - Real-world testing: Hospital deployment
   
   Monitoring:
   - Continuous performance tracking
   - Drift detection (when model performance drops)
   - Feedback loops (learn from corrections)
   - Explainability dashboard
   
   Safety:
   - Adversarial robustness testing
   - Edge case handling
   - Graceful degradation
   - Audit trails for all predictions


Q: What did you learn from this project?

A: Key learnings to mention:
   1. Model architecture design matters
      - Encoder/decoder balance
      - Skip connections and batch norm importance
   2. Checkpoint loading and debugging
      - Tools to inspect state_dict
      - Importance of verification
   3. End-to-end ML workflow
      - From model definition to deployment
      - Training to inference
   4. Testing and validation
      - Creating diagnostic tools
      - Generating synthetic data for testing
   5. Problem-solving approach
      - Systematic debugging
      - Creating tools to understand problems
   6. PyTorch ecosystem depth
      - Many libraries, utilities, best practices


================================================================================
TIPS FOR ANSWERING INTERVIEW QUESTIONS
================================================================================

1. Structure your answers:
   - Start with simple explanation
   - Add technical details
   - Provide examples
   - Discuss alternatives/tradeoffs
   - End with your choice and reasoning

2. Admit what you don't know:
   - "I haven't tried that, but I would..."
   - "Good question, I'd need to research..."
   - Don't bluff, interviewers respect honesty

3. Ask clarifying questions:
   - "Are you asking about model architecture or deployment?"
   - Shows you think deeply

4. Use concrete examples:
   - Reference your project
   - Mention specific lines of code
   - Show you understand implementation

5. Discuss tradeoffs:
   - Nothing is universally best
   - "It depends on..."
   - Shows mature thinking

6. Reference your debugging process:
   - "I created diagnose_checkpoint.py to..."
   - Shows problem-solving skills


================================================================================
END OF ADVANCED QUESTIONS
================================================================================
