================================================================================
ECG DIGITIZER APP - COMPLETE WORKFLOW EXPLANATION
================================================================================

This document traces exactly how data flows through the entire system from
user request to model response, including all intermediate processing steps.


================================================================================
PART 1: HIGH-LEVEL PROJECT PURPOSE
================================================================================

PROJECT GOAL:
Convert ECG panel images (scanned/photographed paper ECGs) into digital 
waveform signals (time-series data). 

INPUT: 12 ECG panel images (RGB, 128x128 each)
OUTPUT: 12 ECG lead waveforms (10,250 time points each, CSV downloadable)

USE CASE:
Medical professionals have paper ECG records that need digitization for 
electronic health records. This app automates the manual process of 
reading paper and re-recording into digital format.

CHALLENGE:
- Model must interpret subtle ink patterns, grid lines, etc.
- Reconstruct smooth, physiologically realistic waveforms
- Handle variations in image quality, scanning angles, contrast


================================================================================
PART 2: SYSTEM ARCHITECTURE
================================================================================

Three main components:

1. FRONTEND (Streamlit Web UI)
   └─ predict.py
      ├─ User uploads 12 ECG panel images
      ├─ Displays uploaded images
      ├─ Shows predicted waveforms
      └─ Offers CSV download

2. BACKEND (Model Inference)
   └─ app.py
      ├─ ResNetPanelEncoder: Extract features from images
      ├─ Fusion Layer: Combine 12 panels into latent representation
      ├─ UNet1DDecoder: Generate waveforms from latent code
      └─ ECGPredictor: Orchestrates everything

3. MODEL STATE
   └─ resnet_unet_best.pth
      └─ Trained weights (165 tensors, ~50MB)


================================================================================
PART 3: USER JOURNEY - STEP BY STEP
================================================================================

USER INTERACTION FLOW:
====================

STEP 1: USER OPENS WEB APP
─────────────────────────
Command: python -m streamlit run predict.py

What happens:
1. Streamlit starts web server (localhost:8501)
2. Browser loads page with Streamlit UI
3. predict.py runs, calls st.set_page_config() → "ECG Digitizer"
4. Display title: "ECG Waveform Digitizer"
5. Display file uploader widget


STEP 2: USER UPLOADS 12 IMAGES
──────────────────────────────
User action: Click upload → Select 12 ECG panel images

Code location: predict.py, lines ~50-60
```python
uploaded_files = st.file_uploader(
    "Upload 12 ECG panels", 
    type="png", 
    accept_multiple_files=True
)
```

What Streamlit does:
- Creates file upload button
- Browser sends files as binary data to Streamlit server
- Streamlit receives files in memory (RAM)
- Files NOT saved to disk yet


STEP 3: VALIDATE AND LOAD IMAGES
─────────────────────────────────
Code location: predict.py, lines ~70-90
```python
if len(uploaded_files) == 12:
    images = []
    for file in uploaded_files:
        img = Image.open(file)  # PIL loads image to RAM
        images.append(img)
```

What happens:
- Check user uploaded exactly 12 files
- PIL (Python Imaging Library) reads each file from memory
- Each image decoded from PNG → RGB arrays
- Image shape checked: must be valid
- Images stored in Python list (RAM)


STEP 4: DISPLAY UPLOADED IMAGES
────────────────────────────────
Code location: predict.py, lines ~95-105
```python
st.write("Uploaded Images:")
for idx, img in enumerate(images):
    st.image(img, caption=f"Panel {idx+1}", width=150)
```

What happens:
- Streamlit displays each image in browser grid
- User can visually verify images uploaded correctly
- No model processing yet - just display


STEP 5: USER CLICKS "PREDICT" BUTTON
─────────────────────────────────────
Code location: predict.py, lines ~110-115
```python
if st.button("Predict Waveforms"):
    # Model inference starts here
```

What happens:
- Browser sends click event to Streamlit server
- Streamlit detects button press
- Enters prediction block


================================================================================
PART 4: BACKEND - MODEL INFERENCE PIPELINE
================================================================================

From button click to prediction output.


STAGE 1: MODEL LOADING
──────────────────────
Code location: predict.py, lines ~25-40
```python
@st.cache_resource
def load_model():
    model = ResNetUNetDigitizer()
    checkpoint = torch.load("resnet_unet_best.pth")
    model.load_state_dict(checkpoint, strict=False)
    model.eval()
    return model

model = load_model()
```

What happens:
1. @st.cache_resource decorator:
   - First time: Load model once, cache in memory
   - Subsequent times: Reuse cached model (fast)
   - Survives across user requests

2. ResNetUNetDigitizer() constructor (app.py):
   - Creates empty model architecture
   - No weights yet, just structure
   - ~11M parameters in ResNet18 encoder
   - ~5M parameters in decoder
   - Total: ~16M parameters

3. torch.load("resnet_unet_best.pth"):
   - Reads checkpoint file from disk (50MB)
   - Deserializes to Python dict
   - Keys: "encoder.layer1.0.conv1.weight", etc. (165 keys total)
   - Values: weight tensors (FP32 floats)

4. model.load_state_dict(checkpoint, strict=False):
   - Matches checkpoint keys to model keys
   - strict=False: Skip keys that don't match (robust loading)
   - Copies weight values into model parameters
   - model.weight = checkpoint['encoder.layer1.0.conv1.weight']
   - Repeats for all 165 keys

5. model.eval():
   - Disables dropout (only train mode)
   - Freezes batch norm statistics
   - Model ready for inference (deterministic)

Result: Model loaded with trained weights, ready to process images


STAGE 2: IMAGE PREPROCESSING
─────────────────────────────
Code location: app.py, ECGPredictor.preprocess() method
```python
def preprocess(self, images):
    transform = transforms.Compose([
        transforms.Resize((128, 128)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5]*3, std=[0.5]*3)
    ])
    tensors = [transform(img) for img in images]
    return torch.stack(tensors)  # Shape: (12, 3, 128, 128)
```

What happens for each of 12 images:

1. Resize to 128×128:
   - Bilinear interpolation
   - Consistent input size
   
2. ToTensor():
   - Convert PIL Image → torch.Tensor
   - RGB values: [0-255] → [0-1]
   - Shape: (3, 128, 128) for each image

3. Normalize mean=[0.5], std=[0.5]:
   - Standardize to mean≈0, std≈1
   - Formula: (pixel - 0.5) / 0.5
   - Expected distribution: ResNet saw ImageNet data with this norm

4. Stack all 12:
   - (3, 128, 128) × 12 → (12, 3, 128, 128)
   - Batch dimension added
   - Ready for GPU/CPU processing

Result: Preprocessed tensor ready for model


STAGE 3: ENCODER - FEATURE EXTRACTION
─────────────────────────────────────
Code location: app.py, ResNetPanelEncoder class
```python
class ResNetPanelEncoder(nn.Module):
    def __init__(self):
        self.resnet = models.resnet18(pretrained=False)
        self.proj = nn.Linear(512, 512)  # Optional projection
    
    def forward(self, x):
        # x shape: (12, 3, 128, 128)
        # Process each panel through ResNet18
        features = self.resnet(x)
        return features  # Shape: (12, 512)
```

What happens:

1. Input: (12, 3, 128, 128) tensor
   - 12 ECG panel images
   - 3 RGB channels
   - 128×128 pixel resolution each

2. ResNet18 processes each panel:
   - Conv layer: 3 channels → 64 channels, stride 2
   - Batch norm + ReLU
   - MaxPool: 128×128 → 64×64
   - Residual block 1: 64→64 channels, 64×64 → 64×64
   - Residual block 2: 64→128 channels, 64×64 → 32×32
   - Residual block 3: 128→256 channels, 32×32 → 16×16
   - Residual block 4: 256→512 channels, 16×16 → 8×8
   - AdaptiveAvgPool2d: 8×8 → 1×1
   - Linear layer: 512 output neurons
   
   Each panel goes through this independently:
   (3, 128, 128) → (512,) feature vector

3. Output: (12, 512)
   - 12 feature vectors, 512-dimensional each
   - Each vector encodes "what patterns does this panel have?"
   - Numbers abstract (not human-interpretable)

Result: 12 panels compressed to 12×512 feature vectors


STAGE 4: FUSION LAYER - COMBINE PANELS
───────────────────────────────────────
Code location: app.py, ResNetUNetDigitizer class
```python
class ResNetUNetDigitizer(nn.Module):
    def __init__(self):
        self.encoder = ResNetPanelEncoder()
        # Fusion layers
        self.fusion_fc1 = nn.Linear(512, 256)
        self.fusion_fc2 = nn.Linear(256, 256)
        self.decoder = UNet1DDecoder()
    
    def forward(self, x):
        # x: (12, 3, 128, 128)
        features = self.encoder(x)  # (12, 512)
        
        # Average across 12 panels
        pooled = features.mean(dim=0)  # (512,)
        
        # Pass through FC layers
        latent = self.fusion_fc1(pooled)  # (256,)
        latent = F.relu(latent)
        latent = self.fusion_fc2(latent)  # (256,)
        
        return latent
```

What happens:

1. Start: 12 feature vectors (12, 512)

2. Mean pooling across panels:
   - Average each of 512 dimensions across 12 panels
   - Panel 1's dim 0: [0.1, 0.2, ..., 0.5] → 0.35
   - Panel 2's dim 0: [0.2, 0.3, ..., 0.6] → 0.45
   - ... Panel 12
   - Average: (0.35+0.45+...)/12
   - Result: single 512-d vector
   - Information loss: all panel-specific details discarded
   
   Why: Converts 12 different panels to 1 unified representation
   Problem: Order doesn't matter (permutation invariant)

3. First FC layer (Linear 512→256):
   - 512 inputs, 256 neurons
   - W matrix: (256, 512)
   - output = W @ input + b
   - Shape: (512,) → (256,)
   - Activation: ReLU (max(0, x))

4. Second FC layer (Linear 256→256):
   - Same size transformation
   - Learns to refine latent representation
   - Shape: (256,) → (256,)

Result: Latent vector (256,)
- Compressed representation of all 12 panels
- Input to decoder


STAGE 5: DECODER - WAVEFORM GENERATION
───────────────────────────────────────
Code location: app.py, UNet1DDecoder class
```python
class UNet1DDecoder(nn.Module):
    def __init__(self):
        # Linear projection: 256 → (12, 256)
        self.project = nn.Linear(256, 12*256)
        
        # Upsampling blocks
        self.up1 = nn.ConvTranspose1d(256, 128, k=4, s=2, p=1)
        self.up2 = nn.ConvTranspose1d(128, 64, k=4, s=2, p=1)
        self.up3 = nn.ConvTranspose1d(64, 32, k=4, s=2, p=1)
        self.up4 = nn.ConvTranspose1d(32, 16, k=4, s=2, p=1)
        
        # Final output layer
        self.final = nn.Conv1d(16, 12, kernel_size=1)
    
    def forward(self, latent):
        # latent: (256,)
        x = self.project(latent)  # (3072,) = 12*256
        x = x.view(1, 12, 256)    # (1, 12, 256)
        
        # Upsample 5 times: 256 → 512 → 1024 → 2048 → 4096 → 10250
        x = self.up1(x)  # (1, 128, 512)
        x = F.relu(x)
        x = self.up2(x)  # (1, 64, 1024)
        x = F.relu(x)
        x = self.up3(x)  # (1, 32, 2048)
        x = F.relu(x)
        x = self.up4(x)  # (1, 16, 4096)
        x = F.relu(x)
        
        # Additional upsampling or padding to reach 10250
        if x.shape[-1] < 10250:
            x = F.pad(x, (0, 10250 - x.shape[-1]))
        
        x = self.final(x)  # (1, 12, 10250)
        return x
```

What happens:

1. Input: Latent vector (256,)
   - Compressed information from all 12 panels
   - "Abstract concept of all panels together"

2. Linear projection 256 → 12×256:
   - Expand to initial decoder state
   - Shape: (3072,) then reshape to (12, 256)
   - 12 channels (one per lead), 256 time steps
   - Similar resolution to encoder feature maps

3. ConvTranspose1d blocks (upsampling):
   
   Block 1: (12, 256) → (128 channels, 512 time steps)
   - Transpose convolution with stride=2
   - Doubles time resolution
   - Reduces channels
   - Learn smooth interpolation patterns
   
   Block 2: (128, 512) → (64 channels, 1024 time steps)
   - 512 → 1024 time points
   
   Block 3: (64, 1024) → (32 channels, 2048 time steps)
   - 1024 → 2048
   
   Block 4: (32, 2048) → (16 channels, 4096 time steps)
   - 2048 → 4096
   
   After all blocks: 4096 time steps, target is 10250
   - Padding adds: 10250 - 4096 = 6154 zeros
   - Or interpolate to exact size

4. Final Conv1d (1×1 kernel):
   - 16 channels → 12 channels
   - Output: (12, 10250)
   - 12 ECG leads, 10,250 time points each
   - These are the predicted waveforms!

Result: 12 waveforms, each 10,250 time points
- Lead I, II, III, aVR, aVL, aVF, V1-V6
- Time resolution: 0.001 seconds (1kHz sampling)
- Total time: 10.25 seconds of ECG


STAGE 6: POST-PROCESSING & OUTPUT
──────────────────────────────────
Code location: predict.py, lines ~150-180
```python
output = model(input_tensor)  # (1, 12, 10250)
output = output.squeeze(0).detach().numpy()  # (12, 10250)

# Extract selected lead
lead_idx = st.selectbox("Select lead", range(12))
waveform = output[lead_idx]  # 10250 time points

# Create figure
fig, ax = plt.subplots(figsize=(14, 4))
time_axis = np.arange(len(waveform)) * 0.001  # seconds
ax.plot(time_axis, waveform, linewidth=0.8)
ax.set_xlabel("Time (seconds)")
ax.set_ylabel("Voltage (mV)")
ax.set_title(f"Lead {['I','II','III','aVR','aVL','aVF','V1','V2','V3','V4','V5','V6'][lead_idx]}")
st.pyplot(fig)
```

What happens:

1. Output from model: (1, 12, 10250) tensor
   - Batch size 1
   - 12 leads
   - 10,250 time points each

2. squeeze(0): Remove batch dimension → (12, 10250)

3. detach(): Remove gradient tracking (inference, no backprop)

4. numpy(): Convert torch tensor → NumPy array
   - Now in Python, not GPU

5. User selects lead (dropdown):
   - Choices: I, II, III, aVR, aVL, aVF, V1-V6
   - e.g., select "Lead II" → lead_idx = 1
   - Extract: output[1] → array of 10,250 values

6. Create time axis:
   - 10,250 time points × 0.001 sec = 10.25 seconds
   - Matplotlib x-axis: [0, 0.001, 0.002, ..., 10.249]

7. Plot in Streamlit:
   - ax.plot(time_axis, waveform)
   - Matplotlib figure → PNG image
   - st.pyplot() displays in browser

8. Display statistics:
   - Min voltage: np.min(waveform)
   - Max voltage: np.max(waveform)
   - Mean: np.mean(waveform)
   - User sees quantitative info


STAGE 7: CSV EXPORT
───────────────────
Code location: predict.py, lines ~185-200
```python
# Create DataFrame
df = pd.DataFrame(output.T, columns=lead_names)
df['Time_sec'] = np.arange(len(df)) * 0.001

# Convert to CSV
csv_str = df.to_csv(index=False)

# Download button
st.download_button(
    label="Download as CSV",
    data=csv_str,
    file_name="ecg_digitized.csv",
    mime="text/csv"
)
```

What happens:

1. Prepare DataFrame:
   - Rows: 10,250 time points
   - Columns: 12 leads + time
   - data[0] = [V_lead1_t0, V_lead2_t0, ..., V_lead12_t0, 0.000]
   - data[1] = [V_lead1_t1, V_lead2_t1, ..., V_lead12_t1, 0.001]
   
2. Convert to CSV format:
   - String representation
   - Comma-separated values
   - Header row with column names

3. Create download button:
   - Browser gets "Download" button
   - Clicking sends CSV to user's Downloads folder
   - File: ecg_digitized.csv

Result: User gets spreadsheet-ready format


================================================================================
PART 5: DATA FLOW SUMMARY
================================================================================

USER INTERACTION:
┌─────────────────────────────────────────────────────────────────┐
│ 1. User opens app in browser (localhost:8501)                   │
│ 2. Streamlit displays file upload widget                        │
│ 3. User uploads 12 ECG panel images                             │
│ 4. Images displayed in browser for verification                 │
│ 5. User clicks "Predict" button                                 │
└─────────────────────────────────────────────────────────────────┘
                           ↓
BACKEND PROCESSING:
┌─────────────────────────────────────────────────────────────────┐
│ 1. Load Model (ResNetUNetDigitizer)                             │
│    └─ Load weights from resnet_unet_best.pth                    │
│                                                                  │
│ 2. Preprocess Images                                             │
│    └─ Resize to 128×128, normalize, stack to (12,3,128,128)    │
│                                                                  │
│ 3. Feature Extraction (Encoder)                                 │
│    └─ ResNet18: (12,3,128,128) → (12,512)                      │
│                                                                  │
│ 4. Fusion                                                        │
│    └─ Mean pool + FC layers: (12,512) → (256,)                 │
│                                                                  │
│ 5. Waveform Generation (Decoder)                                │
│    └─ UNet1D: (256,) → (12,10250)                              │
│                                                                  │
│ 6. Post-processing                                              │
│    └─ Convert to NumPy, extract selected lead                  │
└─────────────────────────────────────────────────────────────────┘
                           ↓
FRONTEND DISPLAY:
┌─────────────────────────────────────────────────────────────────┐
│ 1. Plot waveform in Streamlit                                   │
│ 2. Show statistics (min, max, mean)                             │
│ 3. Display download button for CSV                              │
│ 4. User downloads ecg_digitized.csv                             │
└─────────────────────────────────────────────────────────────────┘


================================================================================
PART 6: MODEL BEHAVIOR - HOW IT WORKS
================================================================================

WHAT THE MODEL LEARNED:
━━━━━━━━━━━━━━━━━━━━━━━

During training (not your responsibility):
1. Model saw thousands of (ECG panel images, ground truth waveforms) pairs
2. Learned to recognize:
   - Ink patterns in images correspond to voltage spikes
   - Grid lines indicate scale/timing
   - Different panel positions contain different leads
   - Waveform shapes typical for healthy/abnormal rhythms

3. Compressed this knowledge into 16M parameters

INFERENCE PROCESS:
━━━━━━━━━━━━━━━━━

Input: "12 paper ECG panel images"
         └─ Encoder asks: "What patterns do I see?"
            └─ Outputs: Feature vectors (12×512)
               └─ Fusion layer asks: "How do these panels relate?"
                  └─ Outputs: Combined understanding (256,)
                     └─ Decoder asks: "Generate smooth waveforms matching this understanding"
                        └─ Outputs: 12 waveforms (12×10250)


WHY WAVEFORMS LOOK SIMILAR:
━━━━━━━━━━━━━━━━━━━━━━━━━━

Issue: Different images produce very similar outputs
Root cause: Model training data may have limited diversity
- Only saw specific types of ECGs
- ECG waveforms naturally similar for healthy patients
- Encoder/decoder compression loses information

Not a bug - expected behavior from limited training data


================================================================================
PART 7: TECHNICAL DETAILS
================================================================================

FILE SIZES & PARAMETERS:
━━━━━━━━━━━━━━━━━━━━━━━

Model Architecture:
- ResNet18 encoder: 11,000,000 parameters
- Fusion layers: 200,000 parameters
- UNet1D decoder: 5,000,000 parameters
- Total: 16,200,000 parameters (trainable weights)

Checkpoint file (resnet_unet_best.pth):
- Size: ~62 MB
- Contains: 165 tensors
  └─ Conv weights, biases, batch norm stats, etc.
- Format: PyTorch native (binary)

Memory usage:
- Loading model: ~200 MB (weights + computation buffers)
- Processing batch: Additional 50-100 MB
- Total RAM needed: ~300 MB


PROCESSING TIME:
━━━━━━━━━━━━━━━━

Typical latencies (CPU):
- Model loading: 2-3 seconds (first time only)
- Image preprocessing: 50-100 ms
- Encoder forward: 200-300 ms (ResNet18)
- Fusion: 10 ms
- Decoder forward: 100-200 ms (upsampling)
- Total inference: 350-600 ms
- Post-processing: 20 ms
- Plotting: 100 ms

Total user-perceived latency: ~500-750 ms (0.5-0.75 seconds)

On GPU (NVIDIA):
- Would be 10-50x faster (5-50 ms inference)


NUMERICAL PRECISION:
━━━━━━━━━━━━━━━━━

All computations: FP32 (32-bit float)
- Range: ±3.4 × 10^38
- Precision: ~7 decimal digits
- Used for all weights, activations, gradients

Output waveform values:
- Typical range: -10 to +10 mV (millivolts)
- Actual values: floating point
- Precision: sufficient for medical interpretation


================================================================================
PART 8: ERROR HANDLING & EDGE CASES
================================================================================

WHAT IF USER UPLOADS WRONG NUMBER OF IMAGES?
─────────────────────────────────────────────
Code: if len(uploaded_files) != 12: st.error("Upload exactly 12 images")

Action: Display error, don't process


WHAT IF IMAGE IS WRONG FORMAT?
──────────────────────────────
Code: Image.open(file) raises exception

Action: Caught in try-except, show error message


WHAT IF IMAGE IS WRONG SIZE?
──────────────────────────────
Code: transforms.Resize((128, 128)) handles it

Action: Resize to 128×128 (can distort if not square)


WHAT IF CHECKPOINT NOT FOUND?
─────────────────────────────
Code: torch.load("resnet_unet_best.pth")

Action: FileNotFoundError, crashes app


WHAT IF WEIGHTS DON'T LOAD?
───────────────────────────
Code: model.load_state_dict(checkpoint, strict=False)

Action: strict=False means skip missing keys, continue


WHAT IF MODEL PRODUCES NaN?
───────────────────────────
Code: Usually from training, not inference

But could happen if:
- Weight values corrupted
- Numerical instability in upsampling

Action: Output would be all NaN, user sees empty plot


================================================================================
PART 9: COMPLETE REQUEST LIFECYCLE EXAMPLE
================================================================================

Timeline of processing 12 uploaded images:

TIME    LOCATION        ACTION
────────────────────────────────────────────────────────────────
0ms     Browser         User clicks "Predict" button
5ms     predict.py      Streamlit detects button, enters prediction block
10ms    app.py          Check model in cache
20ms    (RAM)           Model already loaded, use cached version
25ms    app.py          Call ECGPredictor.predict(images)
30ms    app.py          Preprocess: (12,3,128,128) created
50ms    app.py          Encoder forward: ResNet18 processes images
250ms   app.py          Encoder output: (12,512) features
260ms   app.py          Fusion: mean pooling, FC layers
270ms   app.py          Latent vector: (256,)
280ms   app.py          Decoder forward: upsampling 5 blocks
480ms   app.py          Decoder output: (12,10250) waveforms
490ms   app.py          Post-process: squeeze, to_numpy
510ms   predict.py      Extract lead, create matplotlib figure
530ms   predict.py      st.pyplot() renders in browser
550ms   Browser         User sees waveform plot displayed
560ms   predict.py      Display download button
600ms   User ready for next action

TOTAL TIME: ~600ms visible to user


================================================================================
PART 10: FLOW DIAGRAM
================================================================================

                    ┌─────────────────────┐
                    │   USER'S BROWSER    │
                    │  (localhost:8501)   │
                    └──────────┬──────────┘
                               │
                    (Upload 12 PNG images)
                               │
                    ┌──────────▼──────────┐
                    │   STREAMLIT APP     │
                    │    (predict.py)     │
                    └──────────┬──────────┘
                               │
                    (Validate: 12 images?)
                               │
                    ┌──────────▼──────────────────────┐
                    │   PREPROCESS (predict.py)       │
                    │  Resize 128×128, Normalize      │
                    │  Output: (12,3,128,128) tensor  │
                    └──────────┬─────────────────────┘
                               │
                    ┌──────────▼──────────────────────┐
                    │   ENCODER (app.py)              │
                    │   ResNet18 backbone             │
                    │   Input:  (12,3,128,128)        │
                    │   Output: (12,512)              │
                    └──────────┬─────────────────────┘
                               │
                    ┌──────────▼──────────────────────┐
                    │   FUSION LAYER (app.py)         │
                    │   Mean pool + FC layers         │
                    │   Input:  (12,512)              │
                    │   Output: (256,) latent vector  │
                    └──────────┬─────────────────────┘
                               │
                    ┌──────────▼──────────────────────┐
                    │   DECODER (app.py)              │
                    │   UNet1D upsampling             │
                    │   Input:  (256,)                │
                    │   Output: (12,10250)            │
                    │   ↓ (12 leads, 10250 timepoints)
                    └──────────┬─────────────────────┘
                               │
                    ┌──────────▼──────────────────────┐
                    │   POST-PROCESS (predict.py)     │
                    │   Convert to NumPy              │
                    │   Extract user-selected lead    │
                    └──────────┬─────────────────────┘
                               │
                    ┌──────────▼──────────────────────┐
                    │   DISPLAY (predict.py)          │
                    │   Plot waveform                 │
                    │   Show statistics               │
                    │   CSV download button           │
                    └──────────┬─────────────────────┘
                               │
                    ┌──────────▼──────────────────────┐
                    │   USER'S BROWSER                │
                    │   Sees waveform plot            │
                    │   Can download CSV              │
                    └─────────────────────────────────┘


================================================================================
PART 11: KEY TAKEAWAYS
================================================================================

1. USER REQUEST FLOW:
   Image Upload → Validation → Preprocessing → Model Inference → Display → Download

2. MODEL PIPELINE:
   Encoder (recognize patterns) → Fusion (combine) → Decoder (generate waveforms)

3. DATA TRANSFORMATION:
   (12 images) → (12×512 features) → (256 latent) → (12×10250 waveforms)

4. TECHNOLOGY STACK:
   - Frontend: Streamlit (web framework)
   - Backend: PyTorch (deep learning)
   - Model: ResNet + UNet architecture
   - State: resnet_unet_best.pth checkpoint

5. RESPONSE TIME:
   ~600ms from click to display (human perceivable)

6. ASSUMPTIONS:
   - User uploads exactly 12 images
   - Images are valid ECG panels
   - Model weights loaded correctly
   - Sufficient RAM/CPU available

7. LIMITATIONS:
   - Output quality depends on training data diversity
   - No confidence scoring (model always outputs)
   - No validation of physiological realism
   - Single-threaded (no concurrent requests)


================================================================================
END OF WORKFLOW EXPLANATION
================================================================================
